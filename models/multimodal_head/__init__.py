from .att import SequenceAttention
from .cat import ConcatHead
from .sum import ResidualHead
from .self_att import SelfAttention
from .transformer import Transformer1L, Transformer3L